# Faculty Finder â€“ Data Engineering Pipeline

## Overview
**Faculty Finder** is an end-to-end **Data Engineering project** that builds a structured pipeline to extract, clean, store, and serve faculty information from a university website.  
The system converts unstructured web data into a **queryable, API-driven dataset**, enabling future use cases such as **faculty recommendation systems, internship discovery, and chatbot-based search**.

This repository focuses on the **Data Engineering lifecycle**, covering ingestion, transformation, storage, and serving layers using industry-aligned practices.

---

## Problem Statement
Faculty information is typically scattered across multiple web pages and directories in unstructured HTML format.  
Students face difficulty identifying relevant faculty members based on research interests, academic background, or expertise areas.

Before applying NLP or semantic search, a **reliable and reproducible data pipeline** is required to extract, clean, and organize this information into a structured format.

---

## Solution
This project implements a **production-style ETL pipeline** that:

- Scrapes faculty data from multiple directories
- Normalizes and cleans inconsistent web data
- Stores structured data in a relational database
- Exposes read-only APIs for downstream applications

The pipeline is modular, idempotent, and designed for safe re-execution.

---

## Architecture

University Website<br>
â†“<br>
Web Scraping (Requests + BeautifulSoup)<br>
â†“<br>
Raw JSON Data<br>
â†“<br>
Data Cleaning & Transformation (Pandas)<br>
â†“<br>
Clean CSV Dataset<br>
â†“<br>
SQLite Database<br>
â†“<br>
FastAPI (Read-Only APIs)<br>

---

## ğŸ§© Pipeline Components

### 1ï¸ Ingestion
- Scrapes multiple faculty categories:
  - Faculty
  - Adjunct Faculty
  - Distinguished Professors
  - Professors of Practice
- Handles pagination and inconsistent HTML structures
- Collects profile-level data from faculty pages
- Stores raw scraped output in JSON format

---

### 2ï¸ Transformation
The transformation layer prepares high-quality, analysis-ready data:

- Email and phone number normalization
- UTF-8 encoding cleanup
- Handling missing values
- Tokenization of research areas
- Generation of searchable **research tags**
- Creation of a natural-language **bio** field
- Creation of a `combined_text` column for future NLP usage

The transformed dataset is exported as CSV and JSON.

---

### 3ï¸ Storage
- SQLite used for lightweight, relational storage
- Schema-driven table design
- Idempotent schema creation
- Safe re-runs without schema conflicts
- Structured columns for analytics and APIs

---

### 4ï¸ Serving (API Layer)
A **FastAPI-based read-only API** exposes faculty data for consumption.

| Endpoint | Description |
|--------|------------|
| `GET /faculty` | Fetch all faculty records |
| `GET /faculty/{id}` | Fetch faculty by ID |
| `GET /faculty/search/?tag=<keyword>` | Search faculty by research tag |

Swagger UI is available at:
http://127.0.0.1:8000/docs


---

## ğŸ“ Repository Structure

Assignment1/<br>
â”‚<br>
â”œâ”€â”€ api/<br>
â”‚   â””â”€â”€ main.py<br>
â”‚<br>
â”œâ”€â”€ data/<br>
â”‚   |â”€â”€ daiict_faculty_transformed.json<br>
|   |â”€â”€ daiict_faculty_transformed.csv<br>
|   â””â”€â”€ daiict_all_faculty_raw.json<br>
â”‚<br>
â”œâ”€â”€ db/<br>
â”‚   â”œâ”€â”€ schema.sql<br>
â”‚   â””â”€â”€ faculty_finder.db<br>
â”‚<br>
â”œâ”€â”€ logs/<br>
â”‚   â””â”€â”€ llm_logs.md<br>
â”‚<br>
â”œâ”€â”€ load_to_sqlite.py<br>
â”œâ”€â”€ all_faculty_scrapper.py<br>
â”œâ”€â”€ daiict_faculty_transformed.py<br>
â”œâ”€â”€ run_schema.py<br>
â””â”€â”€ README.md<br>

---

## â–¶ï¸ How to Run

### 1ï¸ Create Virtual Environment & Install Dependencies
```bash
python -m venv venv
# Linux/Mac: source venv/bin/activate
# Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2ï¸ Scrape Faculty Data
```bash
python scraper/all_faculty_scrapper.py
```
### 3ï¸ Transform Data
```bash
python transform/daiict_faculty_transformed.py
```

### 4ï¸ Load Data into Database
```bash
python load_to_sqlite.py
```

### 5ï¸ Start API Server
```bash
uvicorn main:app --reload
```
---

## ğŸ“ Learning Outcomes

- End-to-end data pipeline design
- Web scraping and data ingestion
- Data cleaning and transformation
- Schema design and relational storage
- API development using FastAPI

- Modular and maintainable code organization

